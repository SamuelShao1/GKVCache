services:
  tgi-server:
    build:
      context: ./tgi_server
    image: tgi-server
    container_name: tgi-server
    ports:
      - "8080:80"
    shm_size: "1g"
    runtime: nvidia
    environment:
      - HF_MODEL_ID=TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:  
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  semantic-proxy:
    build:
      context: ./tgi_intercept
    image: semantic-proxy
    container_name: semantic-proxy
    ports:
      - "9000:9000"
    depends_on:
      - tgi-server
