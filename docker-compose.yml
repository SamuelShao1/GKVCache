services:
  tgi-server:
    build: 
      context: ./tgi-gkv-server/text-generation-inference
      dockerfile: Dockerfile
    container_name: tgi-server
    ports:
      - "8080:80"
    shm_size: "1g"
    runtime: nvidia
    volumes:
      - "${PWD}/data:/data"
      - ${PWD}/tgi-gkv-server/cache/kv_cache.py:/opt/conda/lib/python3.11/site-packages/text_generation_server/cache/kv_cache.py
      - ${PWD}/tgi-gkv-server/cache/__init__.py:/opt/conda/lib/python3.11/site-packages/text_generation_server/cache/__init__.py
      - ${PWD}/tgi-gkv-server/models/model.py:/opt/conda/lib/python3.11/site-packages/text_generation_server/models/model.py
      - ${PWD}/tgi-gkv-server/models/causal_lm.py:/opt/conda/lib/python3.11/site-packages/text_generation_server/models/causal_lm.py
      - ${PWD}/tgi-gkv-server/server.py:/opt/conda/lib/python3.11/site-packages/text_generation_server/server.py
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >
      --model-id mistralai/Mistral-7B-Instruct-v0.1
      --trust-remote-code
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  semantic-proxy:
    build:
      context: ./semantic-processor/intercept
    image: semantic-proxy
    container_name: semantic-proxy
    ports:
      - "9000:9000"
    depends_on:
      - tgi-server
    volumes:
      - ./tgi_stats:/app/statistics
    environment:
      - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - TGI_SERVER_URL=http://tgi-server:80